{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine learning on a high level is made up of supervised and unsupervised learning\n",
    "#Within supervised learning, there is classification and regression\n",
    "#Classisfication problems are where the target is a categorical value\n",
    "#Regression problems are where the target is a numerical value\n",
    "#Logistic regression while it has regression in its name is an algorithm for solving classification problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At first, the Titanic dataset will be used and subsequently, the breast cancer dataset\n",
    "#In the Titanic dataset which is stored in a csv file, the Survived column is the target\n",
    "#The Survived column is a list of 1's and 0's where 1 means the passenger survived and 0 means didn't survive\n",
    "#The remaining columns, called features, are the information about the passenger which are used to predict the target\n",
    "#The breast cancer dataset is built into scikit-learn wherein each datapoint has measurements from an image of a breast mass\n",
    "#and whether or not it's cancerous\n",
    "#The target is a 1d numpy array of 1's and 0's where 0 means malignant and 1 means benign\n",
    "#The goal will be to use these measurements to predict if the mass is cancerous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification graphically\n",
    "#Start with only two features (Fare and Age) for simplicity\n",
    "#On the x-axis, the passenger's fare and on the y-axis their age\n",
    "#The plotted dots can have one of two colors - yellow, representing passengers that survived and purple, who didn't survive\n",
    "#The task of a linear model is to find the line that best separates the two classes\n",
    "#For example, if a passenger's datapoint lies on the right side of the line, survival is predicted\n",
    "#if on the left side of the line, they didn't survive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_124500.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equation for the line\n",
    "#A line is defined by an equation in the following form\n",
    "#0 = ax + by + c, the values a, b, c are the coefficients which control where the line is\n",
    "#Any three values will define a unique line\n",
    "#Let a = 1, b = -1 and c = -30\n",
    "#So, 0 = (1)x + (-1)y + (-30), is a unique line with the three coefficients being: 1, -1, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall data is being plotted with Fare of the passenger on the x-axis and Age on the y-axis\n",
    "#To draw a line from an equation, two points that are on the line are needed\n",
    "#So, the point (30, 0) if plugged into the equation - (Fare 30, Age 0) - lies on the line\n",
    "#30 - 0 - 30 = 0\n",
    "#Likewise, The (50, 20) - (Fare 50, Age 20)\n",
    "#50 - 20 -30 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the same line/equation, determine which side of the line a passenger lies based on their data - (Fare 100, Age 20)\n",
    "#0 = (1)x + (-1)y - 30\n",
    "#(1)100 + (-1)20 - 30 = 100 20 - 30 = 50\n",
    "#Since the resultant value is positive, the point is on the right side of the line, predicting the passenger survived\n",
    "#Take another passenger - (Fare 10, Age 50)\n",
    "#(1)10 + (-1)50 - 30 = -70\n",
    "#Since this value is negative, the point lies on the left side of the line, predicting the passenger didn't survive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_123720.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression is a way of mathematically finding the best line\n",
    "#Logistic Regression gives not just a prediction but also a probability, a value between 0 and 1\n",
    "# The equation for calculating this score is given by the function called the sigmoid\n",
    "#The equation for the line is in the form 0 = ax +by + c (x is the Fare, y is the Age, \n",
    "#a, b, c are the coefficients that we control)\n",
    "#The number e is the mathematical constant, approximately 2.71828"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'IMG_20230815_135339.jpg' style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate how good our line is, we need to score whether our predictions (in terms of predicted probability) are correct\n",
    "#The likelihood will be a value between 0 and 1, the higher the value the better the line is\n",
    "#The likelihood equation rewards if prediction is correct otherwise penalizes\n",
    "#In the likelihood equation, p is the predicted probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_140726.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the predicted probability p is 0.25 and the passenger didn't survive, the score would be (1 - p) 0.75 (good) (rewarded)\n",
    "#If the predicted probability p is 0.25 and the passenger survived, the score would be (p) 0.25 (bad) (penalized)\n",
    "#If there are 4 data points, the total score is computed by multiplying the individual scores of each data point\n",
    "#Thus, different lines are compared to determine the best one\n",
    "#0.25 * 0.75 * 0.6* 0.8 = 0.09\n",
    "#The value is always going to be really small as it is the likelihood that the model predicts everything perfectly\n",
    "#A perfect model would have a predicted probability of 1 for all positive cases and 0 for all negative cases\n",
    "#The likelihood is how the possible choices of a best fit line are scored and compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_140659.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses</th>\n",
       "      <th>Parents/Children</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  Siblings/Spouses  Parents/Children  \\\n",
       "0         0       3    male  22.0                 1                 0   \n",
       "1         1       1  female  38.0                 1                 0   \n",
       "2         1       3  female  26.0                 0                 0   \n",
       "3         1       1  female  35.0                 1                 0   \n",
       "4         0       3    male  35.0                 0                 0   \n",
       "\n",
       "      Fare   male  \n",
       "0   7.2500   True  \n",
       "1  71.2833  False  \n",
       "2   7.9250  False  \n",
       "3  53.1000  False  \n",
       "4   8.0500   True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build a logistic regression model with scikit-learn\n",
    "#prep data with pandas\n",
    "import pandas as pd\n",
    "#load the Titanic dataset stored as a csv file\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "#make all columns numerical by creating boolean column for Sex which originally is categorical - male/female\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "#check the first few lines of the resultant pandas DataFrame with all the columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, True, 22.0, 1, 0, 7.25],\n",
       "       [1, False, 38.0, 1, 0, 71.2833],\n",
       "       [3, False, 26.0, 0, 0, 7.925],\n",
       "       ...,\n",
       "       [3, False, 7.0, 1, 2, 23.45],\n",
       "       [1, True, 26.0, 0, 0, 30.0],\n",
       "       [3, True, 32.0, 0, 0, 7.75]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take all the relevant features and create a numpy array called X\n",
    "import numpy as np\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "#take the target (the Survived column) and store it in a variable y\n",
    "y = df['Survived'].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.01615949, -0.01549065]]), array([-0.51037152]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#instantiate the class\n",
    "model = LogisticRegression()\n",
    "#The previously prepared data can now be used to train the model using the fit method\n",
    "#It takes two arguments: X (the features as a 2d numpy array) and y (the target as a 1d numpy array)\n",
    "#For simplicity, build a Logistic Regression model using just the Fare and Age columns/features\n",
    "X = df[['Fare', 'Age']].values\n",
    "y = df['Survived'].values\n",
    "#use the fit method to build the model\n",
    "model.fit(X, y)\n",
    "#fitting/training the model means using the data to choose a line of best fit\n",
    "#check the coefficients with the coef_ and intercept_ attributes\n",
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this values mean that the equation is as follows:\n",
    "#0 = 0.01615949x + -0.01549065y + -0.51037152\n",
    "#the line drawn on the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_145304.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recall only 2 features have been used\n",
    "#now, rebuild the model with all of the features\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "#take the target (the Survived column) and store it in a variable y\n",
    "y = df['Survived'].values\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "#make predictions\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the first passenger in the dataset is: [3, True, 22.0, 1, 0, 7.25]\n",
    "model.predict([[3, True, 22.0, 1, 0, 7.25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model's prediction for the first 5 rows\n",
    "model.predict(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify with target values\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True, False,\n",
       "       False,  True,  True, False,  True,  True, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True, False,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True, False,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True, False,\n",
       "        True, False,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True, False,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False,  True, False, False,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True, False, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False, False,  True, False,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True, False,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True, False,  True,  True, False,  True,  True, False, False,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False,  True,  True, False,  True, False,  True,\n",
       "        True,  True,  True, False,  True, False,  True, False, False,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True, False,  True, False,  True,  True,\n",
       "        True,  True,  True, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True, False, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False, False,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False, False,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False,  True, False,  True, False,  True,  True,\n",
       "       False,  True, False,  True,  True,  True,  True, False, False,\n",
       "        True,  True,  True, False,  True,  True, False,  True,  True,\n",
       "        True,  True,  True, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True, False,  True, False,  True,  True, False,\n",
       "       False,  True, False,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True, False,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True, False, False, False,  True, False,  True,\n",
       "       False,  True, False, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True, False,  True, False,  True,  True,\n",
       "        True, False,  True,  True,  True, False,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True, False,\n",
       "       False,  True, False,  True,  True,  True,  True,  True, False,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "       False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False, False,  True, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False, False,  True,  True,\n",
       "        True,  True, False,  True, False,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "       False,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True, False,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True, False,  True,  True])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score the model\n",
    "#accuracy score gives a sense of how good the model is by counting the number of datapoints it predicts correctly\n",
    "#create an array that has the predicted y values\n",
    "y_pred = model.predict(X)\n",
    "#create an array of boolean values of whether or not the model predicted each passenger correctly\n",
    "y == y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the number of 'true' values\n",
    "(y == y_pred).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the total number of passengers\n",
    "y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8049605411499436"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute the accuracy score\n",
    "(y == y_pred).sum()/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8049605411499436"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the same result using score method\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Introduce the breast cancer dataset\n",
    "#load the dataset and take a peak at the data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer_data = load_breast_cancer()\n",
    "#the object returned and stored in the cancer_data variable is an object similar to a Python dictionary\n",
    "cancer_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry \\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 3 is Mean Radius, field\\n        13 is Radius SE, field 23 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine DESCR which gives a detailed description of the dataset\n",
    "cancer_data['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "        1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "        8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "        8.758e-02],\n",
       "       ...,\n",
       "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "        7.820e-02],\n",
       "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "        1.240e-01],\n",
       "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are 30 features, 569 datapoints, and the target is either Malignant (cancerous) or Benign (not cancerous)\n",
    "#there is additional info and are several features that are calculated based on other columns\n",
    "#the process of figuring out what additional features to calculate is feature engineering\n",
    "#pull the feature and target data out of the cancer_data object\n",
    "cancer_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_data['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#column/feature names\n",
    "cancer_data['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a pandas DataFrame with all the feature data\n",
    "df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the target data - 1d numpy array of 1's and 0's\n",
    "cancer_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_data['target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#interpret the 1's and 0's - 0 means malignant and 1 means benign\n",
    "cancer_data['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add the target data to the pandas DataFrame\n",
    "df['target'] = cancer_data['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANTON CHAKMA\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build a logistic regression model\n",
    "#build the feature matrix X\n",
    "X = df[cancer_data.feature_names].values\n",
    "#build the target array y\n",
    "y = df['target'].values\n",
    "#create a Logistic Regression object\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a Convergence Warning is encountered which means the model needs more time to find the optimal solution\n",
    "#one option is to increase the number of iterations\n",
    "#or switch to a different solver - the solver is the algorithm that the model uses to find the equation of the line\n",
    "#among the possible solvers, choose one\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict for the first datapoint\n",
    "#the predict method takes a 2d array\n",
    "model.predict([X[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9595782073813708"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check model performance\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model evaluation in depth\n",
    "#evaluation metrics\n",
    "#Accuracy is a straightforward metric if the classes are evenly split but is misleading if the classes are imbalanced\n",
    "#It's not only how many of the positive datapoints the model predicts correctly but also how many of the negative datapoints\n",
    "#Confusion Matrix (or Error Matrix or Table of Confusion) produces all the important values and fully describes how a model\n",
    "#performs on a dataset, though is difficult to use to compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix is a table showing four values:\n",
    "#Datapoints predicted positive that are actually positive (True Positives)\n",
    "#Datapoints predicted positive that are actually negative (False Positives)\n",
    "#Datapoints predicted negative that are actually positive (False Negatives)\n",
    "#Datapoints predicted negative that are actually negative (True Negatives)\n",
    "#In the Titanic dataset, there are 887 passengers, 342 survived (positive) and 545 didn't survive (negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_195547.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first and fourth are the datapoints, the model predicted correctly and the second and third are the datapoints\n",
    "#the model predicted incorrectly\n",
    "#compute the accuracy using the confusion matrix - the accuracy is the number of datapoints predicted correctly divided by\n",
    "#the total number of datapoints\n",
    "#(233+480)/233+65+109+480) = 713/887 = 80.38% (the same value as computed earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The confusion matrix is described as follows\n",
    "#The four values of the confusion matrix (TP, TN, FP, FN) are used to compute several different metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_204314.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two commonly used metrics for classification are precision and recall\n",
    "#Both can be defined using quadrants from the cofusion matrix\n",
    "#Precision is a measure of how precise the model is with its positive predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_211121.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the confusion matrix for the model for the Titanic dataset, calculate the precision\n",
    "#precision = 233/(233+65) = 0.7819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall is the percent of positive cases that the model predicts correctly\n",
    "#Recall is a measure of how many of the positive cases the model can recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_211901.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the confusion matrix for the model for the Titanic dataset, calculate the recall\n",
    "#recall = 233/(233+109) = 0.6813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There will be situations where choosing between increasing the recall (while lowering the precision) or increasing the \n",
    "#precision (and lowering the recall) will be necessitated depending on high cost to false positives for example\n",
    "#There's no hard and fast rule on what values of precision and recall you are shooting for\n",
    "#Sometimes, you would want more of a balance between precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy was an appealing metric because it was a single number\n",
    "#Precision and recall are two varying numbers so it's not always obvious how to choose between two models\n",
    "#The F1 score is the harmonic mean of the precision and recall values so that there is a single score for a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_213231.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the F1 score for the model for the Titanic dataset, given the precision and recall calculated earlier\n",
    "#2(0.7819) (0.6813) / (0.7819 + 0.6813) = 0.7281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit-learn has a separate function built in for each of the metrics - accuracy, precision, recall, F1 score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "#Recall the code from the Titanic dataset\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8049605411499436\n"
     ]
    }
   ],
   "source": [
    "#Each function takes two 1d numpy arrays: the true values of the target and the predicted values of the target\n",
    "print(\"accuracy:\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7734627831715211\n"
     ]
    }
   ],
   "source": [
    "print(\"precision:\", precision_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.6988304093567251\n"
     ]
    }
   ],
   "source": [
    "print(\"recall:\", recall_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.7342549923195083\n"
     ]
    }
   ],
   "source": [
    "print(\"f1 score:\", f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With a single model, the metric values do not tell us a lot\n",
    "#For some problems a value of 60% is good, and for others a value of 90% is good, depending on the difficulty of the problem\n",
    "#The metric values can be used to compare different models to pick the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[475,  70],\n",
       "       [103, 239]], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implement confusion matrix in scikit-learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit-learn reverses the confusion matrix to show the negative counts first\n",
    "#Since negative target values correspond to 0 and positive to 1, scikit-learn has orderd them in this order\n",
    "#Confusion matrix in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_220136.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Typical confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_220119.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The previous model was built with all of the data and it performed well on the same data\n",
    "#This is artificially inflating the numbers as it can lead to overfitting\n",
    "#Overfitting is when the model performs well on the data it has already seen but does not perform well on new data\n",
    "#An overfit model tries to get every single datapoint on the correct side of the line but it misses the essence of the data\n",
    "#The more features there are in the dataset, the more prone the model will be to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_222045.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole dataset: (887, 6) (887,)\n",
      "training set: (665, 6) (665,)\n",
      "test set: (222, 6) (222,)\n"
     ]
    }
   ],
   "source": [
    "#To simulate making predictions on new unseen data, break the dataset into a training set and a test set\n",
    "#Training set is used to build the models, test set is used for evaluating the models\n",
    "#A standard breakdown is to put 70-80% of the data in the training set and 20-30% in the test set\n",
    "#Training and testing in scikit-learn\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(\"whole dataset:\", X.shape, y.shape)\n",
    "print(\"training set:\", X_train.shape, y_train.shape)\n",
    "print(\"test set:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The size of the training set can be changed by using the train_size parameter, e.g., train_test_split(X, y, train_size=0.60)\n",
    "#Build a scikit-learn model using a training set\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8018018018018018"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate the model using the test set\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8018018018018018\n",
      "precision: 0.7846153846153846\n",
      "recall: 0.6296296296296297\n",
      "f1 score: 0.6986301369863014\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(\"accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"precision:\", precision_score(y_test, y_pred))\n",
    "print(\"recall:\", recall_score(y_test, y_pred))\n",
    "print(\"f1 score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accuracy, precision, recall and F1 score values are very similar to the values computed using the entire datase\n",
    "#This is a sign the model is not overfit\n",
    "#If you run the code, you get different scores each time as train test split is done randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train [[3, 3], [1, 1], [4, 4]]\n",
      "X_test [[2, 2]]\n"
     ]
    }
   ],
   "source": [
    "#Use random state to get the same split every time\n",
    "#The random state is also called a seed\n",
    "X = [[1, 1], [2, 2], [3, 3], [4, 4]]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 27)\n",
    "print('X_train', X_train)\n",
    "print('X_test', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Foundations of the ROC (receiver operating characteristic) curve\n",
    "#As regards the trade-off between precision and recall, a logistic regression model has an easy way of shifting between\n",
    "#emphasizing precision and emphasizing recall by altering the logistic regression threshold\n",
    "#A logistic regression model doesn't just return a prediction, it also returns a probability value between 0 and 1\n",
    "#Typically, we say if the value is >= 0.5, the passenger is predicted to survive, and if the value is < 0.5, the passenger\n",
    "#didn't survive - we could choose any threshold between 0 and 1\n",
    "#Logistic regression threshold - if we make the threshold higher, we'll have fewer positive predictions but our positive\n",
    "#predictions are more likely to be correct, which means the precision would be higher and the recall lower\n",
    "#If we make the threshold lower, we'll have more positive predictions - more likely to catch all the positive cases - which \n",
    "#that the recall would be higher and the precision lower\n",
    "#Each choice of a threshold is a different model - an ROC curve is a graph showing all of the possible models and their\n",
    "#performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An ROC Curve is a graph of the sensitivity vs. the specificity - these values demonstrate the same trade-off that precision\n",
    "#and recall demonstrate\n",
    "#Define sensitivity and specificity using the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230815_204314.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sensitivity is another term for the recall, which is the true positive rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230816_124350.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The specificity is the true negative rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230816_124328.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following a train test split of the Titanic dataset, the confusion matrix is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230816_124926.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the sensitivity and specificity\n",
    "#Sensitivity = 61/(61+35) = 61/96 = 0.6354\n",
    "#Specificity = 105/(105+21) = 105/126 = 0.8333\n",
    "#The goal is to maximize these two values, though generally making one larger makes the other lower\n",
    "#It will depend on the situation whether we put more emphasis on sensitivity or specificity\n",
    "#For graphing, the standard is to use the sensitivity and specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity and Specificity in scikit-learn\n",
    "#There are no functions defined in scikit-learn for sensitivity and specificity\n",
    "#Sensitivity is the same as recall - recall of the positive cases\n",
    "#Specificity is the recall of the negative class - it can be got from the sklearn function precision_recall_fscore_support\n",
    "#This function returns 4 arrays, of which the second array is the recall\n",
    "#The recall contains two values - the first is the recall of the negative class and the second the recall of the positive class\n",
    "#In other words, the second value is the standard recall value or sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity: 0.6829268292682927\n",
      "specificity: 0.9214285714285714\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_recall_fscore_support\n",
    "\n",
    "sensitivity_score = recall_score\n",
    "def specificity_score(y_true, y_pred):\n",
    "    p, r, f, s = precision_recall_fscore_support(y_true, y_pred)\n",
    "    return r[0]\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"sensitivity:\", sensitivity_score(y_test, y_pred))\n",
    "print(\"specificity:\", specificity_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46054651, 0.53945349],\n",
       "       [0.88810004, 0.11189996],\n",
       "       [0.13520596, 0.86479404],\n",
       "       [0.62497139, 0.37502861],\n",
       "       [0.73722964, 0.26277036],\n",
       "       [0.85687059, 0.14312941],\n",
       "       [0.84133142, 0.15866858],\n",
       "       [0.10657184, 0.89342816],\n",
       "       [0.1260976 , 0.8739024 ],\n",
       "       [0.42023756, 0.57976244],\n",
       "       [0.92436653, 0.07563347],\n",
       "       [0.83523916, 0.16476084],\n",
       "       [0.13698994, 0.86301006],\n",
       "       [0.90217307, 0.09782693],\n",
       "       [0.60512309, 0.39487691],\n",
       "       [0.79564749, 0.20435251],\n",
       "       [0.17515508, 0.82484492],\n",
       "       [0.46654883, 0.53345117],\n",
       "       [0.24196013, 0.75803987],\n",
       "       [0.89247708, 0.10752292],\n",
       "       [0.284676  , 0.715324  ],\n",
       "       [0.87814618, 0.12185382],\n",
       "       [0.86200177, 0.13799823],\n",
       "       [0.87075274, 0.12924726],\n",
       "       [0.86658854, 0.13341146],\n",
       "       [0.22589824, 0.77410176],\n",
       "       [0.85172042, 0.14827958],\n",
       "       [0.85167655, 0.14832345],\n",
       "       [0.56230341, 0.43769659],\n",
       "       [0.64036583, 0.35963417],\n",
       "       [0.89256704, 0.10743296],\n",
       "       [0.04988471, 0.95011529],\n",
       "       [0.51041576, 0.48958424],\n",
       "       [0.87120551, 0.12879449],\n",
       "       [0.94761335, 0.05238665],\n",
       "       [0.91062425, 0.08937575],\n",
       "       [0.03650844, 0.96349156],\n",
       "       [0.59265923, 0.40734077],\n",
       "       [0.62739715, 0.37260285],\n",
       "       [0.95937487, 0.04062513],\n",
       "       [0.22725309, 0.77274691],\n",
       "       [0.90920918, 0.09079082],\n",
       "       [0.35264528, 0.64735472],\n",
       "       [0.54176816, 0.45823184],\n",
       "       [0.44435363, 0.55564637],\n",
       "       [0.82171299, 0.17828701],\n",
       "       [0.74247723, 0.25752277],\n",
       "       [0.89824854, 0.10175146],\n",
       "       [0.86182953, 0.13817047],\n",
       "       [0.26830514, 0.73169486],\n",
       "       [0.78056544, 0.21943456],\n",
       "       [0.84654302, 0.15345698],\n",
       "       [0.69982868, 0.30017132],\n",
       "       [0.32695807, 0.67304193],\n",
       "       [0.85116684, 0.14883316],\n",
       "       [0.72243803, 0.27756197],\n",
       "       [0.07465492, 0.92534508],\n",
       "       [0.04492801, 0.95507199],\n",
       "       [0.87393719, 0.12606281],\n",
       "       [0.33717682, 0.66282318],\n",
       "       [0.85178182, 0.14821818],\n",
       "       [0.14217312, 0.85782688],\n",
       "       [0.84102671, 0.15897329],\n",
       "       [0.92328978, 0.07671022],\n",
       "       [0.85160812, 0.14839188],\n",
       "       [0.90966476, 0.09033524],\n",
       "       [0.96516899, 0.03483101],\n",
       "       [0.68062885, 0.31937115],\n",
       "       [0.37301708, 0.62698292],\n",
       "       [0.49744143, 0.50255857],\n",
       "       [0.69670171, 0.30329829],\n",
       "       [0.87148845, 0.12851155],\n",
       "       [0.07718485, 0.92281515],\n",
       "       [0.60170151, 0.39829849],\n",
       "       [0.61536682, 0.38463318],\n",
       "       [0.04765444, 0.95234556],\n",
       "       [0.05302545, 0.94697455],\n",
       "       [0.95973996, 0.04026004],\n",
       "       [0.8493124 , 0.1506876 ],\n",
       "       [0.72370258, 0.27629742],\n",
       "       [0.99575382, 0.00424618],\n",
       "       [0.88849983, 0.11150017],\n",
       "       [0.28460482, 0.71539518],\n",
       "       [0.85758227, 0.14241773],\n",
       "       [0.28483401, 0.71516599],\n",
       "       [0.90709055, 0.09290945],\n",
       "       [0.51997105, 0.48002895],\n",
       "       [0.91875679, 0.08124321],\n",
       "       [0.60923514, 0.39076486],\n",
       "       [0.89636128, 0.10363872],\n",
       "       [0.94247854, 0.05752146],\n",
       "       [0.31752833, 0.68247167],\n",
       "       [0.73816467, 0.26183533],\n",
       "       [0.95340162, 0.04659838],\n",
       "       [0.37742281, 0.62257719],\n",
       "       [0.8355878 , 0.1644122 ],\n",
       "       [0.93778967, 0.06221033],\n",
       "       [0.69670171, 0.30329829],\n",
       "       [0.17416148, 0.82583852],\n",
       "       [0.80299354, 0.19700646],\n",
       "       [0.13201835, 0.86798165],\n",
       "       [0.924437  , 0.075563  ],\n",
       "       [0.91885077, 0.08114923],\n",
       "       [0.90372851, 0.09627149],\n",
       "       [0.10674313, 0.89325687],\n",
       "       [0.23086955, 0.76913045],\n",
       "       [0.86616094, 0.13383906],\n",
       "       [0.82705086, 0.17294914],\n",
       "       [0.88855372, 0.11144628],\n",
       "       [0.92050557, 0.07949443],\n",
       "       [0.62238356, 0.37761644],\n",
       "       [0.10211846, 0.89788154],\n",
       "       [0.86956933, 0.13043067],\n",
       "       [0.51857668, 0.48142332],\n",
       "       [0.89507424, 0.10492576],\n",
       "       [0.84642718, 0.15357282],\n",
       "       [0.89254039, 0.10745961],\n",
       "       [0.11524085, 0.88475915],\n",
       "       [0.8668039 , 0.1331961 ],\n",
       "       [0.66905541, 0.33094459],\n",
       "       [0.29332897, 0.70667103],\n",
       "       [0.12007603, 0.87992397],\n",
       "       [0.85677684, 0.14322316],\n",
       "       [0.84114434, 0.15885566],\n",
       "       [0.49063125, 0.50936875],\n",
       "       [0.44640134, 0.55359866],\n",
       "       [0.77480167, 0.22519833],\n",
       "       [0.91062425, 0.08937575],\n",
       "       [0.74856103, 0.25143897],\n",
       "       [0.64318775, 0.35681225],\n",
       "       [0.72220967, 0.27779033],\n",
       "       [0.96090821, 0.03909179],\n",
       "       [0.82171299, 0.17828701],\n",
       "       [0.23215468, 0.76784532],\n",
       "       [0.91394145, 0.08605855],\n",
       "       [0.82642974, 0.17357026],\n",
       "       [0.95064406, 0.04935594],\n",
       "       [0.89656761, 0.10343239],\n",
       "       [0.57170922, 0.42829078],\n",
       "       [0.8861574 , 0.1138426 ],\n",
       "       [0.94355129, 0.05644871],\n",
       "       [0.59883168, 0.40116832],\n",
       "       [0.82485306, 0.17514694],\n",
       "       [0.10489915, 0.89510085],\n",
       "       [0.12055164, 0.87944836],\n",
       "       [0.87597398, 0.12402602],\n",
       "       [0.71190398, 0.28809602],\n",
       "       [0.08343316, 0.91656684],\n",
       "       [0.93670035, 0.06329965],\n",
       "       [0.82770652, 0.17229348],\n",
       "       [0.43972211, 0.56027789],\n",
       "       [0.28838169, 0.71161831],\n",
       "       [0.91730356, 0.08269644],\n",
       "       [0.92597463, 0.07402537],\n",
       "       [0.92610309, 0.07389691],\n",
       "       [0.25730204, 0.74269796],\n",
       "       [0.72341679, 0.27658321],\n",
       "       [0.87823027, 0.12176973],\n",
       "       [0.91512639, 0.08487361],\n",
       "       [0.9141045 , 0.0858955 ],\n",
       "       [0.05760757, 0.94239243],\n",
       "       [0.94078789, 0.05921211],\n",
       "       [0.16335032, 0.83664968],\n",
       "       [0.925245  , 0.074755  ],\n",
       "       [0.94393648, 0.05606352],\n",
       "       [0.94597971, 0.05402029],\n",
       "       [0.10242663, 0.89757337],\n",
       "       [0.62161705, 0.37838295],\n",
       "       [0.68571593, 0.31428407],\n",
       "       [0.89585816, 0.10414184],\n",
       "       [0.04083072, 0.95916928],\n",
       "       [0.18785013, 0.81214987],\n",
       "       [0.45255524, 0.54744476],\n",
       "       [0.67787683, 0.32212317],\n",
       "       [0.892111  , 0.107889  ],\n",
       "       [0.2395365 , 0.7604635 ],\n",
       "       [0.88854227, 0.11145773],\n",
       "       [0.10987873, 0.89012127],\n",
       "       [0.94019641, 0.05980359],\n",
       "       [0.93882567, 0.06117433],\n",
       "       [0.79577688, 0.20422312],\n",
       "       [0.49522488, 0.50477512],\n",
       "       [0.75867117, 0.24132883],\n",
       "       [0.90726994, 0.09273006],\n",
       "       [0.90365293, 0.09634707],\n",
       "       [0.06064463, 0.93935537],\n",
       "       [0.56528425, 0.43471575],\n",
       "       [0.82987296, 0.17012704],\n",
       "       [0.86445257, 0.13554743],\n",
       "       [0.46033941, 0.53966059],\n",
       "       [0.84642718, 0.15357282],\n",
       "       [0.33928004, 0.66071996],\n",
       "       [0.87603435, 0.12396565],\n",
       "       [0.84648287, 0.15351713],\n",
       "       [0.70686053, 0.29313947],\n",
       "       [0.85126362, 0.14873638],\n",
       "       [0.47905797, 0.52094203],\n",
       "       [0.92539645, 0.07460355],\n",
       "       [0.2395365 , 0.7604635 ],\n",
       "       [0.8964387 , 0.1035613 ],\n",
       "       [0.65157867, 0.34842133],\n",
       "       [0.53351568, 0.46648432],\n",
       "       [0.84638806, 0.15361194],\n",
       "       [0.32586013, 0.67413987],\n",
       "       [0.86966221, 0.13033779],\n",
       "       [0.67288837, 0.32711163],\n",
       "       [0.66905541, 0.33094459],\n",
       "       [0.37932476, 0.62067524],\n",
       "       [0.63392071, 0.36607929],\n",
       "       [0.95150918, 0.04849082],\n",
       "       [0.90433746, 0.09566254],\n",
       "       [0.95359143, 0.04640857],\n",
       "       [0.20277608, 0.79722392],\n",
       "       [0.03729443, 0.96270557],\n",
       "       [0.72358398, 0.27641602],\n",
       "       [0.07284682, 0.92715318],\n",
       "       [0.54861338, 0.45138662],\n",
       "       [0.88858239, 0.11141761],\n",
       "       [0.84113813, 0.15886187],\n",
       "       [0.68741854, 0.31258146],\n",
       "       [0.72019757, 0.27980243],\n",
       "       [0.88458615, 0.11541385]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adjust the logistic regression threshold in sklearn\n",
    "#When scikit-learn's predict method is used, it gives 0 and 1 values of the prediction\n",
    "#Behind the scenes however the logistic regression model gets a probability value between 0 and 1 for each datapoint and then\n",
    "#rounding to either 0 or 1\n",
    "#If we want to choose a different threshold besides 0.5, we'll want those probability values\n",
    "model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53945349, 0.11189996, 0.86479404, 0.37502861, 0.26277036,\n",
       "       0.14312941, 0.15866858, 0.89342816, 0.8739024 , 0.57976244,\n",
       "       0.07563347, 0.16476084, 0.86301006, 0.09782693, 0.39487691,\n",
       "       0.20435251, 0.82484492, 0.53345117, 0.75803987, 0.10752292,\n",
       "       0.715324  , 0.12185382, 0.13799823, 0.12924726, 0.13341146,\n",
       "       0.77410176, 0.14827958, 0.14832345, 0.43769659, 0.35963417,\n",
       "       0.10743296, 0.95011529, 0.48958424, 0.12879449, 0.05238665,\n",
       "       0.08937575, 0.96349156, 0.40734077, 0.37260285, 0.04062513,\n",
       "       0.77274691, 0.09079082, 0.64735472, 0.45823184, 0.55564637,\n",
       "       0.17828701, 0.25752277, 0.10175146, 0.13817047, 0.73169486,\n",
       "       0.21943456, 0.15345698, 0.30017132, 0.67304193, 0.14883316,\n",
       "       0.27756197, 0.92534508, 0.95507199, 0.12606281, 0.66282318,\n",
       "       0.14821818, 0.85782688, 0.15897329, 0.07671022, 0.14839188,\n",
       "       0.09033524, 0.03483101, 0.31937115, 0.62698292, 0.50255857,\n",
       "       0.30329829, 0.12851155, 0.92281515, 0.39829849, 0.38463318,\n",
       "       0.95234556, 0.94697455, 0.04026004, 0.1506876 , 0.27629742,\n",
       "       0.00424618, 0.11150017, 0.71539518, 0.14241773, 0.71516599,\n",
       "       0.09290945, 0.48002895, 0.08124321, 0.39076486, 0.10363872,\n",
       "       0.05752146, 0.68247167, 0.26183533, 0.04659838, 0.62257719,\n",
       "       0.1644122 , 0.06221033, 0.30329829, 0.82583852, 0.19700646,\n",
       "       0.86798165, 0.075563  , 0.08114923, 0.09627149, 0.89325687,\n",
       "       0.76913045, 0.13383906, 0.17294914, 0.11144628, 0.07949443,\n",
       "       0.37761644, 0.89788154, 0.13043067, 0.48142332, 0.10492576,\n",
       "       0.15357282, 0.10745961, 0.88475915, 0.1331961 , 0.33094459,\n",
       "       0.70667103, 0.87992397, 0.14322316, 0.15885566, 0.50936875,\n",
       "       0.55359866, 0.22519833, 0.08937575, 0.25143897, 0.35681225,\n",
       "       0.27779033, 0.03909179, 0.17828701, 0.76784532, 0.08605855,\n",
       "       0.17357026, 0.04935594, 0.10343239, 0.42829078, 0.1138426 ,\n",
       "       0.05644871, 0.40116832, 0.17514694, 0.89510085, 0.87944836,\n",
       "       0.12402602, 0.28809602, 0.91656684, 0.06329965, 0.17229348,\n",
       "       0.56027789, 0.71161831, 0.08269644, 0.07402537, 0.07389691,\n",
       "       0.74269796, 0.27658321, 0.12176973, 0.08487361, 0.0858955 ,\n",
       "       0.94239243, 0.05921211, 0.83664968, 0.074755  , 0.05606352,\n",
       "       0.05402029, 0.89757337, 0.37838295, 0.31428407, 0.10414184,\n",
       "       0.95916928, 0.81214987, 0.54744476, 0.32212317, 0.107889  ,\n",
       "       0.7604635 , 0.11145773, 0.89012127, 0.05980359, 0.06117433,\n",
       "       0.20422312, 0.50477512, 0.24132883, 0.09273006, 0.09634707,\n",
       "       0.93935537, 0.43471575, 0.17012704, 0.13554743, 0.53966059,\n",
       "       0.15357282, 0.66071996, 0.12396565, 0.15351713, 0.29313947,\n",
       "       0.14873638, 0.52094203, 0.07460355, 0.7604635 , 0.1035613 ,\n",
       "       0.34842133, 0.46648432, 0.15361194, 0.67413987, 0.13033779,\n",
       "       0.32711163, 0.33094459, 0.62067524, 0.36607929, 0.04849082,\n",
       "       0.09566254, 0.04640857, 0.79722392, 0.96270557, 0.27641602,\n",
       "       0.92715318, 0.45138662, 0.11141761, 0.15886187, 0.31258146,\n",
       "       0.27980243, 0.11541385])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The result is a numpy array with 2 values for each datapoint (e.g., [0.78, 0.22])\n",
    "#The two values sum to 1\n",
    "#The first value is the probability that the datapoint is in the 0 class (didn't survive) and the second is the probability\n",
    "#that the datapoint is in the 1 class (survived)\n",
    "#Extract just the second column of this result with the following numpy syntax\n",
    "model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare these probability values with a particular threshold (for example, 0.75) which will give an array of True/False values\n",
    "#The array of newly predicted target values\n",
    "y_pred = model.predict_proba(X_test)[:, 1] > 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9230769230769231\n",
      "recall: 0.43902439024390244\n"
     ]
    }
   ],
   "source": [
    "#A threshold of 0.75 means we need to be more confident of making a positive prediction\n",
    "#This results in fewer positive predictions and more negative predictions\n",
    "#Now use any scikit-learn metrics from before using y_test as true values and y_pred as predicted values\n",
    "print(\"precision:\", precision_score(y_test, y_pred))\n",
    "print(\"recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the threshold to 0.5 would get us the original logistic regression model\n",
    "#Any other threshold value yields an alternative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each choice of a logistic regression threshold is a different model\n",
    "#An ROC curve shows all of the possible models and their performance\n",
    "#The ROC curve is a graph of the specificity vs. the sensitivity - we build a logistic regression model and then calculate\n",
    "#the specificity and sensitivity for every possible threshold\n",
    "#Note that we actually plot the sensitiviy vs. (1-specificity) which is the standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Scikit_learn has a roc_curve function which takes the true target values and the predicted probabilities from the model\n",
    "#The roc_curve function returns an array of the false positive rates (1-specificity - on x-axis), an array of the true positive\n",
    "#rates (sensitivity - on y-axis) and the thresholds\n",
    "#Build an ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "#plot a diagnal line to help visualize how far the model is from a model that predicts randomly\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('1 - specificity')\n",
    "plt.ylabel('sensitivity')\n",
    "plt.savefig('plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpret the ROC curve\n",
    "#The ROC curve is showing the performance not of a single model, but of many models\n",
    "#Each choice of threshold is a different model\n",
    "#Let's look at the ROC curve with these points highlighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230816_224924.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each point A, B, and C refers to a model with a different threshold\n",
    "#Model A has a sensitivity of 0.6 and a specificity of 0.9 (recall that the graph is showing 1-specificity)\n",
    "#Model B has a sensitivity of 0.8 and a specificity of 0.7\n",
    "#Model C has a sensitivity of 0.9 and a specificity of 0.5\n",
    "#How to choose between these models will depend on the specifics of the situation\n",
    "#The closer the curve gets to the upper left corner, the better the performance\n",
    "#The line should never fall below the diagnal line as that would mean it performs worse than a random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Picking a model from the ROC curve\n",
    "#The ROC curve is a way of helping us choose the ideal threshold for our problem\n",
    "#To finalize a model, a single threshold that will be used to make predictions has to be chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Area under the curve (AUC)\n",
    "#Sometimes, we want to use the ROC curve to compare two different models\n",
    "#A comparison of the ROC curves of two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230816_230700.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The blue curve outperforms the orange one since the blue line is almost always above the orange line\n",
    "#To get an empirical measure of this, we calculate the Area Under the Curve (AUC)\n",
    "#It's a value between 0 and 1, the higher the better\n",
    "#Since the ROC is a graph of all the different Logistic Regression models with different thresholds, the AUC does not \n",
    "#measure the performance of a single model\n",
    "#It gives a general sense of how well the Logistic Regression model is performing\n",
    "#To find a single model, you still need to find the optimal threshold for your problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 AUC score: 0.8189749182115593\n",
      "model2 AUC score: 0.8023655733579398\n"
     ]
    }
   ],
   "source": [
    "#Calculate the AUC using roc_auc_score function in scikit-learn\n",
    "#Build two Logistic Regression models on the Titanic dataset, model1 with 6 features and model2 with just Pclass and male features\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X =df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "model1 = LogisticRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred_proba1 = model1.predict_proba(X_test)\n",
    "print(\"model1 AUC score:\", roc_auc_score(y_test, y_pred_proba1[:, 1]))\n",
    "\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train[:, 0:2], y_train)\n",
    "y_pred_proba2 = model2.predict_proba(X_test[:, 0:2])\n",
    "print(\"model2 AUC score:\", roc_auc_score(y_test, y_pred_proba2[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold Cross Validation\n",
    "#Here's the result of a different random train test split in each row on the same dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230817_120447.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.79279\n",
      "precision: 0.80597\n",
      "recall: 0.62069\n",
      "f1 score: 0.70130\n"
     ]
    }
   ],
   "source": [
    "#Each time the model is run, we get different values for the metrics\n",
    "#The accuracy ranges from 0.79 to 0.84, the precision from 0.75 to 0.81 and the recall from 0.63 to 0.75\n",
    "#The values in the training set are never used to evaluate while it would be unfair to build the model with the training\n",
    "#set and then evaluate with the training set, but we are not getting as full a picture of the model performance as possible\n",
    "#Our goal is to get the best possible measure of our metrics (accuracy, precision, recall and F1 score)\n",
    "#The code\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X =df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "#building the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#evaluating the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"accuracy: {0:.5f}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"precision: {0:.5f}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"recall: {0:.5f}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"f1 score: {0:.5f}\".format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into a training set and test set multiple times for k-fold validation\n",
    "#Depending on our test set, we can get different values for the evaluation metrics\n",
    "#We want to get a measure of how well our model does in general, not just a measure of how well it does on one specific test set\n",
    "#Instead of just taking a chunk of the data as the test set, break the dataset into 5 chunks\n",
    "#Assume there are 200 datapoints in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230817_151613.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each of these chunks will serve as a test set\n",
    "#When Chunk 1 is the test set, the remaining 4 chunks are used as the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230817_151557.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each of the 5 times we have a test set of 20% (40 datapoints) and a training set of 80% (160 datapoints)\n",
    "#Every datapoint is in exactly 1 test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build and evaluate with multiple training and test sets\n",
    "#For each training set, we build a model and evaluate it using associated test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230817_152335.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We report the accuracy as the mean of the 5 values\n",
    "#(0.83+0.79+0.78+0.80+0.75)/5 = 0.79\n",
    "#You will only see values this different when you have a small dataset\n",
    "#With large datasets we often just do a training and test set for simplicity\n",
    "#The process of creating multiple training and test sets is called k-fold cross validation\n",
    "#The k is the number of chunks we split the dataset into\n",
    "#The standard number is 5\n",
    "#The goal in cross validation is to get accurate measures for our metrics - accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final model choice in k-fold cross validation\n",
    "#The 5 models were built just for evaluation purposes, so that we can report the metric values\n",
    "#The best possible model is going to be a model that uses all of the data\n",
    "#So, we keep track of our calculated values for our evaluation metrics and then build a model using all of the data\n",
    "#It's worth using a little extra to make sure the right values are reported for evaluation and decision making\n",
    "#Computation power for building a model can be a concern when the dataset is large in which case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0, 3, 4, 5]), array([1, 2])),\n",
       " (array([1, 2, 3, 4]), array([0, 5])),\n",
       " (array([0, 1, 2, 5]), array([3, 4]))]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#k-fold Cross Validation in sklearn\n",
    "#Scikit_learn has already implemented the code to break the dataset into k chunks and create k training and test sets\n",
    "#For simplicity, take a dataset with just 6 datapoints and 2 features and a 3-fold cross validation on the dataset\n",
    "#Take the first 6 rows from the Titanic dataset and use just the Age and Fare columns\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#data prep\n",
    "X = df[['Age', 'Fare']].values[:6]\n",
    "y = df['Survived'].values[:6]\n",
    "\n",
    "#instantiate a KFold class object which takes two parameters: n_splits (this is k, the number of chunks to create)\n",
    "#and shuffle (whether or not to randomize the order of the data)\n",
    "#It's generally good practice to shuffle the data since you often get a dataset that's in sorted order\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "#The KFold class has a split method that creates the 3 splits for the data\n",
    "list(kf.split(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 4 5] [1 3]\n",
      "[1 2 3 4] [0 5]\n",
      "[0 1 3 5] [2 4]\n"
     ]
    }
   ],
   "source": [
    "#The KFold class split method returns the indices that are in each of the splits\n",
    "#The split is done randomly and so different datapoints in the sets each time you run the code\n",
    "for train, test in kf.split(X):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 4]), array([3, 5]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create training and test sets with the folds\n",
    "#first of the 3 train/test split\n",
    "splits = list(kf.split(X))\n",
    "first_split = splits[0]\n",
    "first_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set indices: [0 1 2 4]\n",
      "test set indices: [3 5]\n"
     ]
    }
   ],
   "source": [
    "#the first array is the indices for the training set and the second is the indices for the test set\n",
    "#create these variables\n",
    "train_indices, test_indices = first_split\n",
    "print(\"training set indices:\", train_indices)\n",
    "print(\"test set indices:\", test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "[[22.      7.25  ]\n",
      " [38.     71.2833]\n",
      " [26.      7.925 ]\n",
      " [35.      8.05  ]]\n",
      "y_train [0 1 1 0]\n",
      "X_test\n",
      "[[35.     53.1   ]\n",
      " [27.      8.4583]]\n",
      "y_test [1 0]\n"
     ]
    }
   ],
   "source": [
    "#create an X_train, y_train, X_test, y_test based on those indices\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "#training and test sets in the same format as from train_test_split\n",
    "print(\"X_train\")\n",
    "print(X_train)\n",
    "print(\"y_train\", y_train)\n",
    "print(\"X_test\")\n",
    "print(X_test)\n",
    "print(\"y_test\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7865168539325843"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build a model\n",
    "#the training and test sets can be used to build a model and make prediction\n",
    "#now use the entire dataset as 4 datapoints is not enough to build a decent model\n",
    "#build and score the model on the first fold of a 5-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "splits = list(kf.split(X))\n",
    "train_indices, test_indices = splits[0]\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7584269662921348, 0.8033707865168539, 0.8361581920903954, 0.7796610169491526, 0.8248587570621468]\n",
      "0.8004951437821367\n",
      "0.8049605411499436\n"
     ]
    }
   ],
   "source": [
    "#We've essentially done a single train/test split so far\n",
    "#In order to do a k-fold cross validation, we need to use each of the other 4 splits to build a model and score the model\n",
    "#Loop over all the folds\n",
    "scores = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "    \n",
    "print(scores)\n",
    "print(np.mean(scores))\n",
    "\n",
    "final_model = LogisticRegression()\n",
    "final_model.fit(X, y)\n",
    "print(final_model.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The KFold class is randomly splitting up the data each time, so a different split will result in different scores\n",
    "#Having calculated thte accuracy, we no longer need the 5 different models\n",
    "#For future use, we just want a single model\n",
    "#To get the single best possible model, we build a model on the whole dataset\n",
    "#If asked the accuracy of this model, we use the accuracy calculatd by cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79775281, 0.78651685, 0.77966102, 0.79096045, 0.81355932])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternatively use the cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = LogisticRegression()\n",
    "arr = cross_val_score(model, X, y, cv = 5)\n",
    "cross_val_score(model, X, y, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7936900907763602"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing different models\n",
    "#Evaluation techniques are essential for deciding between multiple model options\n",
    "#Use these techniques to compare 3 models:\n",
    "#- A logistic regression model using all of the features in the dataset\n",
    "#- A logistic regression model using just the Pclass, Age, and Sex columns\n",
    "#- A logistic regression model using just the Fare and Age columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with all features\n",
      "accuracy: 0.7970926172792484\n",
      "precision: 0.762653995250328\n",
      "recall: 0.6964757715402877\n",
      "f1 score: 0.7242894863010727\n",
      "\n",
      "Logistic Regression with Pclass, Sex and Age features\n",
      "accuracy: 0.7937535707484289\n",
      "precision: 0.7503079710144928\n",
      "recall: 0.7015389240155999\n",
      "f1 score: 0.7231125585532594\n",
      "\n",
      "Logistic Regression with Fare and Age features\n",
      "accuracy: 0.6584142702977211\n",
      "precision: 0.6603649068322982\n",
      "recall: 0.2435247381726255\n",
      "f1 score: 0.3532272727272727\n"
     ]
    }
   ],
   "source": [
    "#import the necessary modules\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#prep the data\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "\n",
    "#build the KFold object using 5 splits as that's standard for all of the models to use\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "#create three different feature matrices X1, X2, X3 having the same target y\n",
    "X1 = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "X2 = df[['Pclass', 'male', 'Age']].values\n",
    "X3 = df[['Fare', 'Age']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "#define a function to score the model which uses the KFold object to calculate the accuracy, precision, recall, F1 score for\n",
    "#a Logistic Regression model with the given feature matrix X and target array y\n",
    "def score_model(X, y, kf):\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "        precision_scores.append(precision_score(y_test, y_pred))\n",
    "        recall_scores.append(recall_score(y_test, y_pred))\n",
    "        f1_scores.append(f1_score(y_test, y_pred))\n",
    "    print(\"accuracy:\", np.mean(accuracy_scores))\n",
    "    print(\"precision:\", np.mean(precision_scores))\n",
    "    print(\"recall:\", np.mean(recall_scores))\n",
    "    print(\"f1 score:\", np.mean(f1_scores))\n",
    "        \n",
    "print(\"Logistic Regression with all features\")\n",
    "score_model(X1, y, kf)\n",
    "print()\n",
    "print(\"Logistic Regression with Pclass, Sex and Age features\")\n",
    "score_model(X2, y, kf)\n",
    "print()\n",
    "print(\"Logistic Regression with Fare and Age features\")\n",
    "score_model(X3, y, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sligtly different results every time the code is run as the k-fold splits are chosen randomly\n",
    "#Choose the best model\n",
    "#The first two models have almost identical scores while the third model has lower scores for all four metrics\n",
    "#Since the first two models have equivalent results, it makes sence to choose the simpler model - with the Pclass, Sex, Age features\n",
    "#Having chosen the best model, build a single final model using all of the data\n",
    "model = LogisticRegression()\n",
    "model.fit(X1, y)\n",
    "#predict\n",
    "model.predict([[3, False, 25, 0, 1, 2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three different combinations of features have been tried\n",
    "#It's possible a different combination would also work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
