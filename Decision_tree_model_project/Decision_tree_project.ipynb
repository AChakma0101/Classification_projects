{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The logistic regression modle is defined by the coefficients which in turn define the line to separate the data\n",
    "#These coefficients are called parameters and so logistic regression is a parametric machine learning algorithm\n",
    "#Decision Trees are an example of a nonparametric machine learning algorithm as it is not defined by parameters\n",
    "#Every machine learning algorithm is either parametric or nonparametric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tree terminology\n",
    "#Decision tree is a flowchart of questions that you answer about a datapoint until you get to a prediction\n",
    "#An example of a Decision Tree for the Titanic dataset\n",
    "##In the Titanic dataset which is stored in a csv file, the Survived column is the target\n",
    "#The Survived column is a list of 1's and 0's where 1 means the passenger survived and 0 means didn't survive\n",
    "#The remaining columns, called features, are the information about the passenger which are used to predict the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_105856.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each of the rectangles is called a node\n",
    "#The nodes which have a feature to split on are called internal nodes\n",
    "#The very first internal node at the top is called the root node\n",
    "#The final nodes where we make the predictions of survived/didn't survive are called lead nodes\n",
    "#Internal nodes all have two nodes below them, and are called the node's children\n",
    "#The terms for trees (root, leaf) come from an actual tree though it's upside down\n",
    "#We also use terms that view the tree as a family tree - child node and parent node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpret a Decision Tree\n",
    "#Let's say we want to know the prediction for a 10 year old male passenger in Pclass 2\n",
    "#At the first node, since the passenger's sex is male, we go to the right child\n",
    "#Then, since their age 10 which is <=13 we go the left child\n",
    "#At the third node, we go the right child since the Pclass is 2\n",
    "#Note that there are no rules that we use every feature or what order we use the features or for a continuous value like age\n",
    "#where we do the split\n",
    "#It is standard in a Decision Tree to have each split just have 2 options\n",
    "#Decision Trees are often favored if you have a non-technical audience since they can easily interpret the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_111432.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How is a Decision Tree constructed?\n",
    "#When building a Decision Tree, we don't just randomly choose which feature to split on first\n",
    "#We want to start by choosing the feature with the most predictive power\n",
    "#For any given dataset, there's a lot of different possible Decision Trees that could be created depending on the order you\n",
    "#use the features\n",
    "#The idea is to mathematically choose the best Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to determine which feature we should split on first, we need to score every possible split so we can choose the split\n",
    "#with the highest score\n",
    "#Ideally, the goal of a split would be to perfectly split the data in such a way that if for instance all women survived the\n",
    "#crash and all men didn't survive, splitting on Sex would be a perfect split\n",
    "#This is rarely going to happen with a real dataset, but we want to get as close to this as possible\n",
    "#The mathematical term we'll be measuring is called information gain\n",
    "#It's a value from 0 to 1 where 0 is the information gain of a useless split and 1 is the information gain of a perfect split\n",
    "#Gini impurity and entropy are used to define information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider a couple possible splits for the Titanic dataset\n",
    "#First try splitting on Age\n",
    "#As it is a numerical feature, we need to pick a threshold to split on\n",
    "#Let the split threshold be Age <= 30 and Age > 30, check how many passengers we have on each side, and how many survived and\n",
    "#how many didn't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_122854.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On both sides, we have about 40% of the passengers surviving, thus we have hardly gained anything from the split\n",
    "#Now, try splitting on Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_122836.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see on the female side the vast majority survived, on the male side, the vast majority didnt' survive\n",
    "#It's a good split\n",
    "#We are going for homogeneity or purity on each side\n",
    "#We'll use the purity values - computed by means of two different mathematical measurements - to calculate information gain\n",
    "#A good choice of a feature to split on, results in each side of the split being pure\n",
    "#A set is pure if all the datapoints belong to the same class (either survived or didn't survive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gini impurity is a measure of how pure a set is, one of the mathematical measurements used to calculate the information gain\n",
    "#We calculate the gini impurity on a subset of our data based on how many datapoints in the set are passengers that survived\n",
    "#and how many are passengers who didn't survive\n",
    "#It will be a value between 0 and 0.5 where 0.5 is completely impure (50% survived and 50% didn't survive) and\n",
    "#0 is completely pure (100% in the same class)\n",
    "#Formula for gini where p is the percent of passengers who survived, so, (1 - p) is the percent of passengers who didn't survive\n",
    "#gini = 2 * p * (1 - p)\n",
    "#The graph of the gini impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_125416.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The maximum value is 0.5 when exactly 50% of the passengers in the set survived\n",
    "#If all passengers survived or didn't survive (percent is 0 or 1), the value is 0\n",
    "#Calculate the gini impurity for the example split on Age<=30 and Age>30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_122854.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On the left\n",
    "#Percent of passengers who survived = 197/(197+328) = 0.3752\n",
    "#Percent of passengers who didn't survive = 1 - 0.375 = 0.6248\n",
    "#Now, calculate the gini impurity\n",
    "#2 * 0.3752 * 0.6248 = 0.4689\n",
    "#We can see that the value is close to 0.5, the maximum value for gini impurity which means the set is impure\n",
    "#On the right, calculate the gini impurity\n",
    "#2 * 145/(145+217) * 217/(145+217) = 04802\n",
    "#The value is also close to 0.5, which means an impure set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, calculate the gini impurity for the example split on Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_122836.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On the left side, for female passengers, calculate the gini impurity\n",
    "#2 * 233/(233+81) * 81/(233+81) = 0.3828\n",
    "#On the right side, for male passengers, calculate the gini impurity\n",
    "#2 * 109/(109+464) * 464/(109+464) = 0.3081\n",
    "#Both values are smaller than the gini values for splitting on Age\n",
    "#We determine that splitting on the Sex feature is a better choice\n",
    "#Right now, we have two values for each potentia split\n",
    "#Information gain will be a way of combining them into a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entropy is another measure of purity\n",
    "#It's a value between 0 and 1 where 1 is completely impure (50% survived and 50% didn't survive) and 0 is completely pure (100%\n",
    "# the same class)\n",
    "#Formula for entropy where p is the percent of passengers who survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_223730.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph of the entropy function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_223705.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The entropy function graph has a similar shape as the gini function, the entropy graph is a little fatter\n",
    "#The maximum value is when 50% of the passengers in our set survived and the minimum value is when either all or none of the\n",
    "#passengers survived\n",
    "#Calculate the entropy values for the same two potential splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_122854.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On the left (Age<=30):\n",
    "#p = 197/(197+328) = 0.3752\n",
    "#Entropy = -(0.375 * log(0.375) + (1 - 0.375) * log(1 - 0.375)) = 0.9546\n",
    "#On the right (Age>30):\n",
    "#p = 145/(145+217) = 0.4006\n",
    "#Entropy = -(0.401 * log(0.401) + (1 - 0.401) * log(1 - 0.401)) = 0.9713\n",
    "#The values are both close to 1, which means the sets are impure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_122836.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On the left (female):\n",
    "#p = 233/(233+81) = 0.7420\n",
    "#Entropy = -(p * log(p) + (1 - p) * log(1 - p)) = 0.8237\n",
    "#On the right (male):\n",
    "#p = 109/(109+464) = 0.1902\n",
    "#Entropy = -(p * log(p) + (1 - p) * log(1 - p)) = 0.7019\n",
    "#These entropy values are smaller than the entropy values above, which means this is a better split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's not obvious whether gini or entropy is a better choice\n",
    "#It often won't make a difference but one can always cross validate to compare a Decision Tree with entropy and a Decision\n",
    "#Tree with gini to see which performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information gain\n",
    "#Now that we have a way of calculating a numerical value for impurity, we can define information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_230400.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H is the impurity measure (either Gini impurity or entropy)\n",
    "#S is the original dataset and A and B are the two sets we are splitting the dataset S into\n",
    "#In the first example above, A is passengers with Age<=30 and B is passengers with Age>30\n",
    "#In the second example, A is female passengers and B is male passengers\n",
    "#|A| means the size of A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Information Gain for the two examples with Gini impurity as the impurity measure\n",
    "#Most of the Gini impurity values are already calculated, though we need to calculate the Gini impurity of th whole set\n",
    "#There are 342 passengers who survived and 545 passengers who didn't survive out of a total of 887 passengers\n",
    "#So the gini impurity is as follows\n",
    "#Gini = 2 * 342/887 * 545/887 = 0.4738"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_122854.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that we have 197+328 = 525 passengers on the left (Age<=30) and 145+217 = 362 passengers on the right (Age>30)\n",
    "#Plugging in the gini impurity values calculated earlier:\n",
    "#Information gain = 0.4738 - 525/887 * 0.4689 - 362/887 * 0.4802 = 0.0003\n",
    "#The value is very small meaning we gain very little from this split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230818_122836.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have 233+81=314 passengers on the left (female) and 109+464=573 passengers on the right(male)\n",
    "#Information gain = 0.4738 - 314/887 * 0.3828 - 573/887 * 0.3081 = 0.1393\n",
    "#The information gain is much better for this split\n",
    "#The work we did was just to compare two splits, we'll need to do the same calculations for every possible split in order to\n",
    "#find the best one\n",
    "#Luckily, we don't have to do the computations by hand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the Decision Tree\n",
    "#To determine how to do the first split, we go over every possible split and calculate the information gain if we used that split\n",
    "#For numerical features like Age, PClass and Fare, we try every possible threshold\n",
    "#Splitting on the Age threshold of 50 means that datapoints with Age<=50 are one group and those with Age>50 are the other\n",
    "#Since there 89 (n) different ages in our dataset, we have 88 (n - 1) different splits to try in the age feature\n",
    "#The same applies for the other numerical features\n",
    "#For each of these splits we calculate the information gain and we choose the split with the highest value\n",
    "#We do the same thing for the next level\n",
    "#Say we did the first split on Sex, now for all the female passengers, we try all of the possible splits for each of the features\n",
    "#choose the one with the highest information gain\n",
    "#We can split on the same feature twice if that feature has multiple possible thresholds\n",
    "#Independently, we do a similar calculation for the male passengers and choose the split with the highest information gain\n",
    "#Thus we may have a different second split for male passengers and female passengers\n",
    "#We continue doing this process until we have no more features to split on\n",
    "#This is a lot of things to try but we just need to throw computation power at it\n",
    "#It does make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An example Decision Tree for the Titanic dataset\n",
    "#Within each internal node, we have the feature and threshold to split on - the number of samples and the distribution of the\n",
    "#same (didn't survive vs. survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230819_123256.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To interpret this,\n",
    "#male <= 0.5\n",
    "#samples = 887\n",
    "#value = [545, 342]\n",
    "#This means that the first split will be on the male column\n",
    "#If the value is <=0.5, meaning the passenger is female, we go to the left child and if the value is >0.5, meaning the\n",
    "#passenger is male, we go to the right child\n",
    "#There are 887 datapoints to start and 545 are negative cases (didn't survive) and 342 are positive (did survive)\n",
    "#The two children of the root node, we can see how many datapoints were sent each way based on splitting on Sex\n",
    "#There are 314 female passengers in our dataset and 573 male passengers\n",
    "#The second split for female passengers is different from the second split for male passengers\n",
    "#The diagram was created with graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make a prediction using the same Decision Tree\n",
    "#Take the values Sex: female, Pclass: 3, Fare: 25, Age: 30\n",
    "#We ask the question at each node and go to the left child if the answer is yes and to the right if the answer is no\n",
    "#We start at the root node\n",
    "#Is the value for the male feature <= 0.5? (This question could also be asked as \"Is the passenger female?\")\n",
    "#Since the answer is yes, we go to the left child\n",
    "#Is the Pclass <= 2.5? Since the answer is no, we go to the right child\n",
    "#Is the Fare <= 23.35? Since the answer is no, we go to the right child\n",
    "#Now, we are at the leaf node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230819_125528.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The leaf node that we end at has the following text:\n",
    "#   27\n",
    "#[24, 3]\n",
    "#This means there are 27 datapoints in our dataset that also land at this leaf node\n",
    "#24 of them didn't survive and 3 of them survived\n",
    "#This means our prediction is that the passenger didn't survive\n",
    "#Because there are no rules as to how the tree is developed, the decision tree asks completely different questions of a female\n",
    "#passenger than a male passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Trees in scikit-learn\n",
    "#Just like with Logistic Regression, scikit-learn has a Decision Tree class\n",
    "#The code for building a Decision Tree model is very similar to building a Logistic Regression model\n",
    "#import the DecisionTreeClassifier class\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load the data to a DataFrame\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "#data prep\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "#do a train/test split using a random_state so that every time we will get the same split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)\n",
    "#create a DecisionTreeClassifier object\n",
    "model = DecisionTreeClassifier()\n",
    "#train/fit the model\n",
    "model.fit(X_train, y_train)\n",
    "#predict\n",
    "model.predict([[3, True, 22, 1, 0, 7.25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "  accuracy: 0.7778772297340188\n",
      "  precision: 0.7127952095095512\n",
      "  recall: 0.7113597799259563\n",
      "Logistic Regression\n",
      "  accuracy: 0.7970354853043865\n",
      "  precision: 0.7618898922983288\n",
      "  recall: 0.6900529617441382\n"
     ]
    }
   ],
   "source": [
    "#The model predicts that the passenger did not survive - the same prediction that the Logistic Regression model gave\n",
    "#Score a Decision Tree model, use k-fold cross validation to get an accurate measure of the metrics and compare the values\n",
    "#with a Logistic Regression model\n",
    "#Use random_state when creating the KFold object so that we will get the same results every time\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "dt_accuracy_scores = []\n",
    "dt_precision_scores = []\n",
    "dt_recall_scores = []\n",
    "lr_accuracy_scores = []\n",
    "lr_precision_scores = []\n",
    "lr_recall_scores = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    dt = DecisionTreeClassifier()\n",
    "    dt.fit(X_train, y_train)\n",
    "    dt_accuracy_scores.append(dt.score(X_test, y_test))\n",
    "    dt_y_pred = dt.predict(X_test)\n",
    "    dt_precision_scores.append(precision_score(y_test, dt_y_pred))\n",
    "    dt_recall_scores.append(recall_score(y_test, dt_y_pred))\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_accuracy_scores.append(lr.score(X_test, y_test))\n",
    "    lr_y_pred = lr.predict(X_test)\n",
    "    lr_precision_scores.append(precision_score(y_test, lr_y_pred))\n",
    "    lr_recall_scores.append(recall_score(y_test, lr_y_pred))\n",
    "    \n",
    "print(\"Decision Tree\")\n",
    "print(\"  accuracy:\", np.mean(dt_accuracy_scores))\n",
    "print(\"  precision:\", np.mean(dt_precision_scores))\n",
    "print(\"  recall:\", np.mean(dt_recall_scores))\n",
    "print(\"Logistic Regression\")\n",
    "print(\"  accuracy:\", np.mean(lr_accuracy_scores))\n",
    "print(\"  precision:\", np.mean(lr_precision_scores))\n",
    "print(\"  recall:\", np.mean(lr_recall_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accuracy and precision of the Logistic Regression model is higher and the recalls of the two models are about the same\n",
    "#The Logistic Regression model performs better, though we may still want to use a Decision Tree for its interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gini vs. Entropy\n",
    "#The default impurity criterion in scikit-learn's Decision Tree algorithm is the Gini Impurity, however, they have also\n",
    "#implemented entropy and you can choose which one you'd like to use when you create the DecisionTreeClassifier object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230819_140351.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - gini\n",
      "accuracy: 0.7564717831524155\n",
      "precision: 0.6809932587370817\n",
      "recall: 0.6890200145899245\n",
      "\n",
      "Decision Tree - entropy\n",
      "accuracy: 0.7554116676188662\n",
      "precision: 0.6824462922834533\n",
      "recall: 0.6840086652099961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To build a Decision Tree that uses entropy, we'll need to set the criterion parameter to entropy\n",
    "#dt = DecisionTreeClassifier(criterion = 'entropy')\n",
    "#Compare a Decision Tree using gini with a Decision Tree using entropy\n",
    "#First create a k-fold split since when we are comparing two models we want them to use the same train/test splits to be fair\n",
    "#Then we do a k-fold cross validation with each of the two possible models\n",
    "#We calculate accuracy, precision and recall for each of the two options\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for criterion in ['gini', 'entropy']:\n",
    "    print(\"Decision Tree - {}\".format(criterion))\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        dt = DecisionTreeClassifier(criterion=criterion)\n",
    "        dt.fit(X_train, y_train)\n",
    "        y_pred = dt.predict(X_test)\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        precision.append(precision_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "    print(\"accuracy:\", np.mean(accuracy))\n",
    "    print(\"precision:\", np.mean(precision))\n",
    "    print(\"recall:\", np.mean(recall))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is very little difference in the performance of Gini vs. Entropy which is expected as they aren't really very different\n",
    "#functions\n",
    "#It's rare to find a dataset where the choice would make a difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Decision Trees\n",
    "#To create a png image of the Decision Tree graph, you can use scikit-learn's export_graphviz function (make sure to install\n",
    "#graphviz first - \"pip install graphviz\")\n",
    "#export the export_graphviz function\n",
    "#from sklearn.tree import export_graphviz\n",
    "#dt is a Decision Tree object and feature_names is a list of the feature names\n",
    "#Graph objects are store as .dot files which can be the GraphViz program\n",
    "#The goal is to save a png image file\n",
    "#We will be able to convert the dot file to a png file, so we first save the dot file to a variable\n",
    "#dot_file = export_graphviz(dt, feature_names=feature_names)\n",
    "#use the graphviz module to convert it to a png image format\n",
    "#import graphviz\n",
    "#graph = graphviz.Source(dot_file)\n",
    "#Finally, we can use the render method to create the file, provide filename and file format, add cleanup to get rid of extra \n",
    "#files we are not interested in\n",
    "#graph.render(filename='tree', format='png', cleanup=True)\n",
    "#There should a file called tree.png on your computer\n",
    "#Here's the code to visualize the tree for the Titanic dataset with just the Sex and Pclass features\n",
    "\n",
    "#from sklearn.tree export export_graphviz\n",
    "#import graphviz\n",
    "#from IPython.display import Image\n",
    "\n",
    "#feature_names = ['Pclass', 'male']\n",
    "#X = df[feature_names].values\n",
    "#y = df['Survived'].values\n",
    "\n",
    "#dt = DecisionTreeClassifier()\n",
    "#dt.fit(X, y)\n",
    "\n",
    "#dot_file = export_graphviz(dt, feature_names=feature_names)\n",
    "#graph = graphviz.Source(dot_file)\n",
    "#graph.render(filename='tree', format='png', cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_20230819_144444.jpg\" style=\"width:400px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overfitting is when we do a good job of building a model for the training set but it doesn't perform well on the test set\n",
    "#Decision Trees are incredibly prone to overfitting\n",
    "#If you let a Decision Tree keep building, it may create a tree that's overfit and doesn't capture the essence of the data\n",
    "#In order to solve these issues, we do what's called pruning the tree - we make the tree smaller with the goal of reducing \n",
    "#overfitting\n",
    "#There are two types of pruning: pre-pruning and post-pruning\n",
    "#In pre-pruning, we have rules when to stop builing the tree, so we stop builing before the tree is too big\n",
    "#In post-pruning we build the whole tree and then we review the tree and decinde which leaves to remove to make the tree smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-pruning\n",
    "#We have a few options for how to limit the tree growth\n",
    "#Max depth: Only grow the tree up to a certain depth, or height of the tree - if the max depth is 3, there will be at most 3\n",
    "#splits for each datapoint\n",
    "#Leaf size: Don't split a node if the number of samples at that node is under a threshold\n",
    "#Number of leaf nodes: Limit the total number of leaf nodes allowed in the tree\n",
    "#Pruning is a balance - if you se the max depth too small, you won't have much of a tree and you won't have any predictive power\n",
    "#This is called underfitting\n",
    "#Similarly, if the leaf size is too large, or the number of leaf nodes too small, you will have an underfit model\n",
    "#There is no hard science as to which pre-pruning method will yield better results - in practice, we try a few different values\n",
    "#for each parameter and cross validate to compare their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-pruning parameters in scikit-learn\n",
    "#Scikit-learn has implemented quite a few techniques for pre-pruning - in particular, max_depth, min_samples_leaf, \n",
    "#and max_leaf_nodes\n",
    "#Prepruning technique 1: We use the max_depth parameter to limit the number of steps the tree can have between the root node\n",
    "#and the leaf nodes\n",
    "#Prepruning technique 2: We use the min_samples_leaf parameter to tell the model to stop building the tree early if the number\n",
    "#of datapoints in a leaf will be below a threshold\n",
    "#Prepruning technique 3: We use max_leaf_nodes to set a limit on the number of leaf nodes in the tree\n",
    "#Here's an example code for creating a Decision Tree with the following properties\n",
    "#dt = DecisionTreeClassifier(max_depth=3, min_samples_leaf=2, max_leaf_nodes=10)\n",
    "#The model can now be trained and tested\n",
    "#You can use as many or as few of the parameters as you'd like\n",
    "#To determine the best values for the pre-pruning parameters, we'll use cross validation to compare several potential options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit-learn has a grid search class called GridSearchCV built in which loops through the different options to cross validate\n",
    "#and compare metrics\n",
    "#GridSearchCV has four parameters:\n",
    "#1. The model (in this case a DecisionTreeClassifier)\n",
    "#2. Param grid - a dictionary of the parameters names and all the possible values\n",
    "#3. What metric to use (default is accuracy)\n",
    "#4. How many folds for k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the param grid variable - give a list of all the possible values for each parameter that we want to try\n",
    "#param_grid = {'max_depth': [5, 15, 25], 'min_samples_leaf': [1, 3], 'max_leaf_nodes': [10, 20, 35, 50]}\n",
    "#New create the grid search object - use the param grid, set the scoring metric to the F1 score, do a 5-fold cross validation\n",
    "#dt = DecisionTreeClassifier()\n",
    "#gs = GridSearchCV(dt, param_grid, scoring='f1', cv=5)\n",
    "#fit the grid search object - it can take a little time to run as it tries every possible combination of the parameters\n",
    "#gs.fit(X, y)\n",
    "#Since we have 3 possible values for max_depth, 2 for min_samples_leaf and 4 for max_leaf_nodes, we have 3*2*4 = 24 different\n",
    "#combinations to try\n",
    "#We use the best_params_ attribute to see which model won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'max_depth': 25, 'max_leaf_nodes': 50, 'min_samples_leaf': 1}\n",
      "best score: 0.7726769086210947\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 15, 25],\n",
    "    'min_samples_leaf': [1, 3],\n",
    "    'max_leaf_nodes': [10, 20, 35, 50]}\n",
    "dt = DecisionTreeClassifier()\n",
    "gs = GridSearchCV(dt, param_grid, scoring='f1', cv=5)\n",
    "gs.fit(X, y)\n",
    "\n",
    "#The best_params_ attribute tells us the winning model\n",
    "print(\"best params:\", gs.best_params_)\n",
    "\n",
    "#The best_score_ attribute tells us the score of the winning model\n",
    "print(\"best score:\", gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generally, if we have multiple models with comparable performance, we'd choose the simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree pros and cons\n",
    "#When talking about how much computation is required for a machine learning algorithm, we separate it into two questions: how\n",
    "#much computation is required to build the model and how much is required to predict\n",
    "#A decision tree is very computationally expensive to build - this is because at every node we are trying every single feature\n",
    "#and threshold as a possible split, we have to calculate the information gain of each of these possible splits each time\n",
    "#Predicting with a decision tree on the other hand is computationally very inexpensive\n",
    "#Generally, we care much more about the computation time for prediction than training which has to preferably happen in real time\n",
    "#Decision Trees can perform decently well depending on the data, though they are prone to overfitting\n",
    "#To remedy the overfitting issues, decision trees generally require some tuning to ge the best possible model\n",
    "#Decision Trees often take work to perform on par with how other models perform with no tuning\n",
    "#Interpretability\n",
    "#The biggest reason people like choosing Decision Trees is because they ar easily interpretable\n",
    "#A non-technical person can interpret a Decision Tree so it's easy to give an explanation of a prediction\n",
    "#Interpretability is the biggest advantage of Decision Trees - it will depend on the situation whether this is important for \n",
    "#your problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
